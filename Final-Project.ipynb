{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project of Machine Learning with Spark\n",
    "\n",
    "#### Author - Rohit Rokde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/u/rrokde/spark-2.4.4-bin-hadoop2.7')\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Project Assignment - ML with Spark\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading\n",
    "\n",
    "data = spark.read.load('/u/rrokde/mls/default of credit card clients1.csv', \n",
    "                       format='csv', header=False, inferSchema=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe structure used for training and evaluation:-\n",
      "root\n",
      " |-- default_payment_next_month: double (nullable = true)\n",
      " |-- scaledFeatures: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Data Transformation\n",
    "\n",
    "#We need to remove the garbage values from the first row.\n",
    "from itertools import islice\n",
    "data = data.rdd.mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if (idx == 0 ) else it).toDF()\n",
    "firstRow = data.head(1)\n",
    "\n",
    "#Second row contains the column names.\n",
    "columnNames = []\n",
    "for i in range(len(firstRow[0])):\n",
    "    columnNames.append( str(firstRow[0][i]).replace(' ', '_') )\n",
    "data = data.rdd.mapPartitionsWithIndex(lambda idx, it: islice(it, 1, None) if (idx == 0 ) else it).toDF()\n",
    "\n",
    "#Next we tidy up the dataframe by setting the right column names and casting column data to the right type\n",
    "for i in range(len(firstRow[0])):\n",
    "    data = data.withColumnRenamed(\"_c\" + str(i), columnNames[i])\n",
    "    data = data.withColumn(columnNames[i],data[columnNames[i]].cast('double'))\n",
    "    # Source: https://forums.databricks.com/questions/9147/how-to-infer-csv-schema-default-all-columns-like-s.html\n",
    "\n",
    "#Now we optimize our data for machine learning. That is extracting features and then normalizing it.\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "#Code taken from 1st assignment\n",
    "df = data\n",
    "list_of_columns = df.columns\n",
    "list_of_columns.pop()     #Remove the target column from feature vector\n",
    "assembler = VectorAssembler().setInputCols(list_of_columns).setOutputCol(\"features\")\n",
    "transformed = assembler.transform(df)\n",
    "scaler = MinMaxScaler(inputCol=\"features\",outputCol=\"scaledFeatures\")\n",
    "scalerModel =  scaler.fit(transformed.select(\"features\"))\n",
    "scaledData = scalerModel.transform(transformed)\n",
    "\n",
    "#Remove unnecessary columns. We have all the data in vectorised column - scaledFeatures\n",
    "for col in scaledData.columns:\n",
    "    if( (col == 'default_payment_next_month') or (col == 'scaledFeatures')):\n",
    "        pass\n",
    "    else:\n",
    "        scaledData = scaledData.drop(col)\n",
    "\n",
    "print(\"Dataframe structure used for training and evaluation:-\")\n",
    "scaledData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Learning\n",
    "\n",
    "\n",
    "# Split the data into training and test sets\n",
    "(trainingData, testData) = scaledData.randomSplit([0.7, 0.3])\n",
    "\n",
    "#Source: https://spark.apache.org/docs/latest/ml-classification-regression.html#binomial-logistic-regression\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"default_payment_next_month\", featuresCol=\"scaledFeatures\")\n",
    "\n",
    "# Chain indexers and tree in a Pipeline\n",
    "#pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "pipeline = Pipeline(stages=[dt])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier accuracy =  81.85%\n"
     ]
    }
   ],
   "source": [
    "# 4. Model evaluation\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"default_payment_next_month\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"DecisionTreeClassifier accuracy = \", str(format( (accuracy)*100, '.2f') ) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of the model is around 82%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some values to verify our results:-\n",
      "+----------+--------------------------+\n",
      "|prediction|default_payment_next_month|\n",
      "+----------+--------------------------+\n",
      "|       0.0|                       0.0|\n",
      "|       0.0|                       0.0|\n",
      "|       0.0|                       0.0|\n",
      "|       0.0|                       0.0|\n",
      "|       0.0|                       0.0|\n",
      "|       0.0|                       0.0|\n",
      "|       0.0|                       0.0|\n",
      "|       0.0|                       0.0|\n",
      "|       0.0|                       0.0|\n",
      "|       0.0|                       0.0|\n",
      "+----------+--------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "print(\"Some values to verify our results:-\")\n",
    "predictions.select(\"prediction\", \"default_payment_next_month\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
